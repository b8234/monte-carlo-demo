# ðŸŽ¯ Monte Carlo Data Observability Demo

> **Enterprise-ready data quality monitoring platform demonstrating production-level data engineering capabilities**

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-latest-red.svg)](https://streamlit.io/)
[![DuckDB](https://img.shields.io/badge/duckdb-latest-yellow.svg)](https://duckdb.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ðŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Quick Start](#-quick-start)
- [Architecture](#-architecture)
- [Dashboard Capabilities](#-dashboard-capabilities)
- [Technical Implementation](#-technical-implementation)
- [Usage Examples](#-usage-examples)
- [Development](#-development)
- [Deployment](#-deployment)
- [Contributing](#-contributing)

## ðŸŒŸ Overview

This project demonstrates a **comprehensive learning platform** for data observability concepts inspired by Monte Carlo Data. Built with modern data stack technologies, it provides hands-on experience with enterprise-level data engineering patterns, real-time monitoring, and AI-powered analysis.

### Learning Objectives

- **Data Observability Mastery**: Understanding multi-dimensional quality scoring, anomaly detection, and monitoring patterns
- **Enterprise SDK Integration**: Hands-on experience with Monte Carlo's pycarlo SDK and API patterns
- **Modern Data Stack Proficiency**: Working with DuckDB, Streamlit, Python, and AI integration
- **Production Development Practices**: Professional deployment, error handling, testing, and documentation
- **Real-time Systems Architecture**: File monitoring, live dashboards, and automated quality assessment

## âœ¨ Features

### ðŸŽ›ï¸ Core Capabilities

- **Real-time Data Quality Monitoring** - Live quality scoring and anomaly detection
- **AI-Powered Analysis** - Natural language insights with OpenAI integration
- **File System Monitoring** - Automated validation and schema drift detection
- **Monte Carlo SDK Integration** - Enterprise-grade observability platform integration

### ðŸš€ Technical Highlights

- **Modern Data Stack**: DuckDB + Streamlit + Python 3.12+
- **Professional Deployment**: Automated setup with zero-configuration
- **Production Patterns**: Comprehensive error handling and logging
- **Enterprise Architecture**: Modular design with clear separation of concerns

## ðŸš€ Quick Start

### Prerequisites

- Python 3.12 or higher
- Git
- 4GB+ available RAM
- Terminal/Command Line access

### One-Command Setup

```bash
# Clone the repository
git clone https://github.com/b8234/monte-carlo-demo.git
cd monte-carlo-demo

# Run automated setup and start dashboard
./setup.sh
```

The setup script will:
1. Create Python virtual environment
2. Install all dependencies
3. Configure environment variables
4. Load sample data
5. Start the dashboard at `http://localhost:8501`

### Alternative Setup Options

```bash
# Full setup without auto-start
./setup.sh setup

# Just start the dashboard
./setup.sh start

# Reset and restart (if needed)
./setup.sh reset

# Verify everything is working
./setup.sh verify

# Clean installation
./setup.sh clean
```

## ðŸ—ï¸ Architecture

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚    â”‚     DuckDB       â”‚    â”‚   File System   â”‚
â”‚   Dashboard     â”‚â—„â”€â”€â–ºâ”‚   Analytics      â”‚â—„â”€â”€â–ºâ”‚   Monitoring    â”‚
â”‚                 â”‚    â”‚    Engine        â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                        â–²                       â–²
         â”‚                        â”‚                       â”‚
         â–¼                        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenAI API    â”‚    â”‚  Monte Carlo     â”‚    â”‚   Sample Data   â”‚
â”‚   Integration   â”‚    â”‚   SDK Client     â”‚    â”‚   (17 files)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Frontend** | Streamlit | Interactive dashboard and visualization |
| **Database** | DuckDB | High-performance analytics engine |
| **Monitoring** | Watchdog | Real-time file system monitoring |
| **Integration** | Monte Carlo SDK | Enterprise observability platform |
| **AI Analysis** | OpenAI API | Natural language data insights |
| **Deployment** | Bash Scripts | Automated setup and process management |

## ðŸ“Š Dashboard Capabilities

### Tab 1: Data Quality Dashboard
- **Real-time Quality Metrics**: Live scoring with configurable thresholds
- **Historical Trend Analysis**: Quality degradation tracking over time
- **Multi-dimensional Scoring**: Completeness, consistency, timeliness, accuracy
- **Interactive Visualizations**: Charts and graphs with drill-down capabilities

### Tab 2: AI Analysis
- **Natural Language Insights**: AI-powered explanations of data patterns
- **Anomaly Detection**: Intelligent identification of quality issues
- **Predictive Analysis**: Forecasting potential data quality problems
- **Business Impact Assessment**: Analysis of quality issues on business metrics

### Tab 3: File Monitoring
- **Real-time File Detection**: Automatic monitoring of new data files
- **Schema Validation**: Immediate detection of schema changes
- **Quality Scoring**: Per-file quality assessment and reporting
- **Error Tracking**: Comprehensive logging of validation failures

### Tab 4: Monte Carlo SDK Integration
- **Overview Section**: Connection status and SDK capabilities
- **Quality Metrics**: Enterprise-grade rule management
- **Incident Management**: Alert configuration and resolution tracking
- **Rule Management**: Custom quality rule creation and monitoring
- **API Patterns**: Production-ready integration examples

## ðŸ”§ Technical Implementation

### Project Structure

```
monte-carlo-demo/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ monte_carlo_dashboard.py    # Main application (40KB+ of code)
â”œâ”€â”€ pycarlo_integration/            # Monte Carlo SDK implementation
â”‚   â”œâ”€â”€ monte_carlo_client.py       # Core SDK client (700+ lines)
â”‚   â”œâ”€â”€ demo.py                     # Demo scenarios
â”‚   â””â”€â”€ examples/                   # Integration examples
â”œâ”€â”€ sample_data/                    # 17 enterprise data scenarios
â”œâ”€â”€ database/                       # DuckDB analytics database
â”œâ”€â”€ tests/                          # Comprehensive test suite
â”œâ”€â”€ setup.sh                       # Professional deployment automation
â”œâ”€â”€ requirements.txt               # Dependency management
â”œâ”€â”€ .env.example                   # Environment configuration template
â””â”€â”€ personal.md                    # Technical presentation guide
```

### Key Design Decisions

**Database Choice**: DuckDB for columnar analytics performance without server overhead

**Architecture Pattern**: Modular monolith with clear separation of concerns

**Quality Algorithm**: Weighted multi-dimensional scoring based on industry best practices

**Integration Strategy**: Facade pattern for enterprise SDK complexity management

**Error Handling**: Defensive programming with graceful degradation

## ðŸ“ˆ Usage Examples

### Basic Quality Monitoring

1. **Start the dashboard**: `./setup.sh start`
2. **Open browser**: Navigate to `http://localhost:8501`
3. **View quality metrics**: Real-time scores and trend analysis
4. **Explore data**: Interactive charts and drill-down capabilities

### Live File Monitoring Demo

```bash
# Create a new data file while dashboard is running
cd sample_data
cat > new_customer_data_$(date +%Y%m%d).csv << EOF
customer_id,revenue,signup_date,status
1001,2500.00,2025-01-30,active
1002,1800.75,2025-01-30,pending
EOF

# Watch the File Monitoring tab for real-time detection
```

### Testing Data Quality Issues

```bash
# Create file with quality problems
cat > problematic_data_$(date +%Y%m%d).csv << EOF
customer_id,revenue,signup_date,email
1003,invalid_amount,2025-13-99,not-an-email
,1800.75,,incomplete@
EOF

# Observe quality score changes and validation alerts
```

### AI Analysis (with OpenAI configured)

1. Edit `.env` file with your OpenAI API key
2. Restart dashboard: `./setup.sh reset`
3. Navigate to AI Analysis tab
4. View natural language insights and recommendations

## ðŸ› ï¸ Development

### Local Development Setup

```bash
# Clone and setup development environment
git clone https://github.com/b8234/monte-carlo-demo.git
cd monte-carlo-demo

# Create development environment
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Start in development mode
streamlit run src/monte_carlo_dashboard.py --server.fileWatcherType auto
```

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run comprehensive test suite
pytest tests/ -v --cov=src/

# Run specific test categories
pytest tests/test_quality_metrics.py -v
pytest tests/test_file_monitoring.py -v
```

### Code Quality

```bash
# Format code
black src/ tests/

# Lint code
flake8 src/ tests/

# Type checking
mypy src/
```

## ðŸš€ Deployment

### Production Deployment

#### Docker Deployment

```bash
# Build Docker image
docker build -t monte-carlo-demo .

# Run container
docker run -p 8501:8501 monte-carlo-demo
```

#### Cloud Deployment (AWS/Azure/GCP)

```bash
# Configure cloud environment variables
export CLOUD_PROVIDER="aws"  # or "azure", "gcp"
export DATABASE_URL="your-cloud-database-url"

# Deploy using cloud-specific scripts
./deploy/deploy-${CLOUD_PROVIDER}.sh
```

### Environment Configuration

Create `.env` file from template:

```bash
cp .env.example .env
# Edit .env with your configuration
```

Required environment variables:

```bash
# Database Configuration
DUCKDB_PATH=database/monte-carlo.duckdb

# Optional: OpenAI Integration
OPENAI_API_KEY=your-api-key-here

# Optional: Monte Carlo Integration  
MONTE_CARLO_API_KEY=your-monte-carlo-key
```

## ðŸ“Š Performance Benchmarks

| Metric | Performance |
|--------|-------------|
| **Dashboard Load Time** | < 3 seconds |
| **Quality Score Calculation** | < 2 seconds |
| **File Processing** | < 1 second per MB |
| **Database Queries** | < 500ms |
| **Memory Usage** | ~100-200MB |
| **Supported File Count** | 100+ files |
| **Database Scale** | GB-scale data |

## ðŸ”’ Security & Compliance

- **Environment Variable Management**: Secure credential handling
- **Audit Logging**: Comprehensive activity tracking
- **Data Encryption**: Secure data handling practices
- **Access Controls**: Role-based permission patterns ready for implementation

## ðŸ¤ Contributing

### Development Workflow

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

### Contribution Guidelines

- Follow existing code style and patterns
- Add tests for new features
- Update documentation as needed
- Ensure all tests pass before submitting PR

## ðŸ“š Documentation

- **[Technical Presentation Guide](personal.md)** - Comprehensive demo and architecture guide
- **[API Documentation](docs/api.md)** - Detailed API reference
- **[Deployment Guide](docs/deployment.md)** - Production deployment instructions
- **[Contributing Guide](docs/contributing.md)** - Development workflow and standards

## ðŸ› Troubleshooting

### Common Issues

**Dashboard won't start**:
```bash
./setup.sh reset
./setup.sh start
```

**Database connection errors**:
```bash
./setup.sh data  # Reload data
./setup.sh verify  # Check status
```

**File monitoring not working**:
```bash
# Check sample_data directory permissions
ls -la sample_data/
chmod 755 sample_data/
```

**Performance issues**:
```bash
# Check system resources
./setup.sh verify
# Restart with clean state
./setup.sh clean && ./setup.sh setup
```

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- **Monte Carlo Data** - For the excellent data observability platform and SDK
- **DuckDB Team** - For the high-performance analytics engine
- **Streamlit Team** - For the intuitive dashboard framework
- **OpenAI** - For the powerful AI analysis capabilities

## ðŸ“ž Contact

**Developer**: GitHub @b8234
**Project Link**: [https://github.com/b8234/monte-carlo-demo](https://github.com/b8234/monte-carlo-demo)

---

*Built with â¤ï¸ for enterprise data engineering excellence*

**Ready to demonstrate production-level data engineering capabilities!** ðŸš€